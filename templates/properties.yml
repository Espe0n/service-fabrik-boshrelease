meta:
  environment: ~
  stemcell: ~
  host: ~
  networks: ~
  compilation: ~
  redirect_uri: ~
  service_definitions: ~
  properties:
  
    ########################
    # SWARM JOB PROPERTIES #
    ########################
    # - required by swarm to find its docker cluster nodes
    swarm:
      # comma separated list of docker cluster nodes, missing nodes are ignored by swarm
      discovery: nodes://10.244.4.3:4243
      # TLS configuration fo swarm manager
      tls: true
      tls_cert: <%= JSON.dump(File.read("../certs/sf.crt")) %>
      tls_key: <%= JSON.dump(File.read("../certs/sf.key")) %>
      tls_verify: true
  
    #########################
    # DOCKER JOB PROPERTIES #
    #########################
    docker:
      # bind to any interface
      tcp_address: 0.0.0.0
      # max open files for Docker daemon process
      ulimit:
        nofile: 1_000_000
      icc: true
      tls: true
      tls_cert: <%= JSON.dump(File.read("../certs/sf.crt")) %>
      tls_key: <%= JSON.dump(File.read("../certs/sf.key")) %>
      tls_verify: true
  
    lvmvd:
      # enable/disable Docker volume quota
      disabled: true
      # activate device mapper instead of default aufs
      # see https://docs.docker.com/reference/commandline/daemon/#daemon-storage-driver-option
      # see http://jpetazzo.github.io/2014/01/29/docker-device-mapper-resize
      # main advantage:    quota on root fs if images are patched for root FS data location
      # main disadvantage: executable and shared library memory not shared among containers
      # WARNING: if changed (in-flight), all containers and all meta data, images, etc. are lost!
      # WARNING: (default) options are applied only once; devicemapper folder must be deleted to apply different options and again, all containers and all meta data, images, etc. are lost!
      #   storage_driver: devicemapper # enforce container quota (default 10 GB)
      #   storage_options:
      #     - "dm.basesize=20G"        # change container quota to new value
      # TLS CONFIGURATION FOR DOCKER NODES
  
    ####################################
    # COMMON PROPERTIES SHARED BY JOBS #
    ####################################
    common:
      # TLS CA CERTIFICATE AND CLIENT KEY PAIRS TO COMMUNICATE SWARM manager AND DOCKER NODES
      tls_cacert: <%= JSON.dump(File.read("../certs/sf-ca.crt")) %>
      tls_client_cert: <%= JSON.dump(File.read("../certs/sf.crt")) %>
      tls_client_key: <%= JSON.dump(File.read("../certs/sf.key")) %>
  
    ##############################
    # ROUTE REGISTRAR PROPERTIES #
    ##############################
    # - required for registration of broker route at router via nats
    route_registrar:
      routes:
      - name: service-fabrik-broker
        registration_interval: 20s
        port: 9292
        tags:
          component: service-fabrik-broker
          env: production
        uris:
        - (( meta.host ))
        health_check:
          name: service-fabrik-broker-health-check
          script_path: '/var/vcap/jobs/service-fabrik-broker/bin/health_check'
          timeout: 5s
  
    ###################
    # NATS PROPERTIES #
    ###################
    # - required for registration of broker route at router via nats
    nats:
      user: nats
      password: nats
      machines:
      - 10.244.0.6
      port: 4222
  
  
    #########################
    # BROKER JOB PROPERTIES #
    #########################
    broker:
      ####################
      # GENERAL SETTINGS #
      ####################
      name: service-fabrik-broker
      username: broker
      password: secret
      skip_ssl_validation: true
      log_level: debug
  
      ##############################
      # EXTERNAL ENDPOINT SETTINGS #
      ##############################
      external:
        port: 9292
        host: (( meta.host ))
        trust_proxy: 2
        cookie_secret: '214a2bed-4988-4841-ae8b-c1b6c29a9de4'
        api_requires_admin_scope: false
        log_event: true
      
      ##############################
      # INTERNAL ENDPOINT SETTINGS #
      ##############################
      internal:
        port: 9293
        ip: 10.244.4.2
        log_event: true
        ssl:
         key: <%= JSON.dump(File.read("../certs/sf.key")) %> 
         cert: <%= JSON.dump(File.read("../certs/sf.crt")) %>
      ####################
      # MONGODB SETTINGS #
      ####################
      mongodb:
        provision: #Defines the plan id of mongodb from the catalog to be provisioned by fabrik
          network_index: 1
        record_max_fetch_count: 100  # Max number of records that can be fetched at a time
        deployment_name: 'service-fabrik-mongodb'
        retry_connect:
          max_attempt: 8 # MaxAttempts of 8 with a backoff factor of 2 leads to last attempt ~ at 4.3 hr from first error occurence time
          min_delay: 120000 # min 2 mins
        backup:
          schedule_interval: 0 12 * * * # Cron expression schedule interval for backup job of service fabrik's internal mongo DB
          status_check_every: 120000 # Interval in milliseconds at which service fabrik's backup job checks for backup operation status with mongo service agent
          backup_timeout_time: 7200000 # Timeout time in milliseconds by which duration the backup job/operation for service fabrik's mongodb must be completed (2 hrs)
        agent:
          version: '1'
          provider:
            container: 'cf.mycf.com-service-fabrik-mongodb'
          auth:
            username: service-fabrik-mongodb-agent
            password: service-fabrik-mongodb-secret
          supported_features:
          - state
          - lifecycle
          - credentials 
          - backup
          - restore
  
      ######################
      # SCHEDULER SETTINGS #
      ######################
      scheduler:
        job_types: ScheduledBackup, ServiceFabrikBackup, ScheduledOobDeploymentBackup, OperationStatusPoller # Comma seperated list of batch job types that are enabled in service fabrik
        process_every: '3 minutes' # Interval at which scheduler will query the database looking for jobs that need to be processed
        max_concurrency: 50 # A number which specifies the max number of batch jobs that can be running at any given moment
        default_concurrency: 20 # A number which specifies the default number of a specific job that can be running at any given moment
        default_lock_lifetime: 900000 # Specifies the default lock lifetime of a batchjob in milliseconds.
        agenda_collection: agendaJobs # Name of the collection in mongodb which is to be used by agendaJS to store the scheduled job meta info
        job_history_retention_in_days: 30 # Retention period in number of days, for which the job run history is maintained.
  
  
      #######################
      # MONITORING SETTINGS #
      #######################
      riemann:
        host: ~
        port: 5555
        prefix: CF
        metrics: metrics
      syslog:
        host: ~
        port: 5514
  
      ###################
      # BACKUP SETTINGS #
      ###################
      backup:
        provider:
          name: openstack
          container: ~
          authUrl: ~
          keystoneAuthVersion: ~
          projectDomainName: ~
          tenantId: ~
          region: ~
          strictSSL: false
          username: ~
          password: ~
        retention_period_in_days: 14
        max_num_on_demand_backup: 2
        backup_restore_status_poller_timeout: 86400000
        backup_restore_status_check_every: 120000
  
      #############################
      # CLOUD-CONTROLLER SETTINGS #
      #############################
      cf:
        url: https://api.bosh-lite.com
        username: admin
        password: admin
  
      ###################
      # DOCKER SETTINGS #
      ###################
      docker:
        url: "https://10.244.4.2:2376" # how to connect the broker to swarm
        allocate_docker_host_ports: true                                 # if true the broker will allocate fixed port mappings for the containers
 
      #####################
      # DIRECTOR SETTINGS #
      #####################
      #bosh director settings
      directors:
      - url: https://192.168.50.4:25555
        name: bosh
        username: admin
        password: admin
        primary: true
        support_create: true
        lock_deployment_max_duration: 86400
        default_task_poll_interval: 10000
        skip_ssl_validation: true
        infrastructure:
          stemcell: (( meta.stemcell ))
          segmentation:
            network_name: service-fabrik-bosh-services
            # calculate capacity based on the network range
            capacity: -1
            offset: 1
            size: 8
          azs:
          - { name: z1, cloud_properties: { name: random } }
          - { name: z2, cloud_properties: { name: random } }
          - { name: z3, cloud_properties: { name: random } }
          vm_types:
          - { name: micro,    cloud_properties: { name: random } }
          - { name: small,    cloud_properties: { name: random } }
          - { name: medium,   cloud_properties: { name: random } }
          - { name: large,    cloud_properties: { name: random } }
          - { name: xlarge,   cloud_properties: { name: random } }
          - { name: 2xlarge,  cloud_properties: { name: random } }
          - { name: 4xlarge,  cloud_properties: { name: random } }
          - { name: 10xlarge, cloud_properties: { name: random } }
          disk_types:
          - { name: hdd_1gb,   disk_size:     1_024 } # minimum size 1GiB (2^30), everything else is similar to hard disk vendors (10^x)
          - { name: hdd_2gb,   disk_size:     2_000 }
          - { name: hdd_4gb,   disk_size:     4_000 }
          - { name: hdd_6gb,   disk_size:     6_000 }
          - { name: hdd_8gb,   disk_size:     8_000 }
          - { name: hdd_10gb,  disk_size:    10_000 }
          - { name: hdd_20gb,  disk_size:    20_000 }
          - { name: hdd_40gb,  disk_size:    40_000 }
          - { name: hdd_60gb,  disk_size:    60_000 }
          - { name: hdd_80gb,  disk_size:    80_000 }
          - { name: hdd_100gb, disk_size:   100_000 }
          - { name: hdd_200gb, disk_size:   200_000 }
          - { name: hdd_400gb, disk_size:   400_000 }
          - { name: hdd_600gb, disk_size:   600_000 }
          - { name: hdd_800gb, disk_size:   800_000 }
          - { name: hdd_1tb,   disk_size: 1_000_000 }
          - { name: hdd_2tb,   disk_size: 2_000_000 }
          - { name: hdd_4tb,   disk_size: 4_000_000 }
          - { name: hdd_6tb,   disk_size: 6_000_000 }
          - { name: hdd_8tb,   disk_size: 8_000_000 }
          networks: (( meta.networks ))
          compilation: (( meta.compilation ))
  
      services: (( meta.service_definitions))
